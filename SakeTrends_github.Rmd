---
title: "Sake Trends on Twitter"
author: "Jordan White"
date: "7/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(twitteR)
library(tm)
library(dplyr)
library(tidyr)
library(readr)
library(tidytext)
library(purrr)
library(wordcloud)
library(SnowballC)
library(hunspell)
library(cld2) # Google's language detector
library(stringr)
library(geojsonio)
library(leaflet)
library(htmltools)
library(jsonlite)

# Set up twitteR
consumer_key <- # You need these from your own account
consumer_secret <- #
access_token <- #
access_secret <- #

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

# Basic search
# tw <- searchTwitter("#sake", n = 3, lang = 'en') # pulls "sake" hash tag
# d <- twListToDF(tw) # converts above to a data frame listing attributes
```
Quick Note: This is a truncation of a much longer Rmd file. The contents here 
lay out some of the initial steps to my analyses.

## Goals  
The main goal here is to see how saké as a hash tag has developed over time. 
After building a basic plot of its usage over time, I'd like to dig deeper and 
look at how it has changed over time in specific locations. Here's a list of 
goals:

*  Use a simple search that is language blind or English specific and find out 
how often #sake has been used over time. 
*  Narrow the search down to those tweets originating in the US, CAN, or MEX. 
*  Plot the above North American tweets against the continental layout. Perhaps 
make it an interactive map that plots by year.

### Requirements  
* A file containing tweets you have collected
* A separate file containing the associated users and their metadata, collected 
by twitteR
* A Japanese stopwords file

### Contents  
* Code for extracting user location from data output by twitteR package
* Geocoding: grabs any (lat, lon) data already in the metadata of user profiles
* Text mining of tweets: initial parsing including removal of stopwords and 
subsetting to remove some bycatch (Muslims tweeting #sake and meaning "for the 
sake of Allah", they probably are not drinking saké)
* Code for a wordcloud and the associated set up: reformatting of some words for 
pretty display and deletion of uninteresting words (numbers etc)

```{r GetUserLocation}
# Use with twitteR to grab user info
# Note: I noticed a HTTP 403 error for some sets of requests if the request was  
# about 100 users in size. Try scanning with 50 users and imposing a 2 sec rest 
# in between calls to prevent network overload.
setwd("~/Documents/Sake_Brewing/")
users <- read.csv("users20190705.csv")
users <- as.character(users$x)

user_data <- rep("None", length(users)) # pre-allocate memory for the output vector
buffer <- rep(NA, 50)
x <- 0
y <- 0

for(var in 1:(length(users) %/% 50)){ # has an edge case at the end, floored
        x <- var*50 - 49 # grab pieces 50 long
        y <- var*50
        buffer <- lookupUsers(users[x:y], includeNA = TRUE) # download fifty users' info
        user_data[x:y] <- buffer
        Sys.sleep(2) # 2 secs
}

edge <- (length(users) %/% 50) *50 + 1 #deal with edge case manually
buffer <- lookupUsers(users[edge:length(users)], includeNA = TRUE)
user_data[edge:length(users)] <- buffer

# extract location info, delete the rest
locations <- sapply(user_data, '[[', "location")# from JilberUrbina and GSee

# This workaround fixes an aspect of unlist, unlist deletes NULL list values,
#   thereby truncating the output vector in NULL locations
extract_locations <- as.character(rep(NA, length(locations)))
for(i in 1:length(locations)){
    if(!is.null(locations[[i]])){
        extract_locations[i] <- locations[[i]]
    } else{
        extract_locations[i] <- NA
    }
}

empty <- grepl("^\\b$", extract_locations) # finds empty strings
extract_locations[empty] <- NA

rm(user_data, locations)
```

```{r Geocoding}
# Need to pull device lat lon info first and remove from consideration
# This will give me more accurate info and avoids errors of Google geocoding
search_key <- "\\d{1,2}\\.\\d{2,6}[,\\s\t]-{0,1}\\d{1,3}\\.\\d{2,6}$"
matches <- grepl(search_key, extract_locations)
#     Explanation of above RegEx: find at least one digit, then period, then
#     two to six digits, a separator, followed by zero or one negative...

# To extract matches or lat lng:
library(stringr)
extract_locations[matches] <- str_extract(extract_locations[matches], "-{0,1}\\d+.\\d+.+\\d+.\\d+$") # grabs the matching digits
#extract_locations[matches][531] <- "14.6056220,121.0147680"
# last line converts a value that was degree,minute,second format

new_users_frame <- tibble("User" = new_users, "Location" = extract_locations)
new_users_frame["lat"] <- NA
new_users_frame["lng"] <- NA
new_users_frame$lat[matches] <- str_extract(extract_locations[matches]
                                            ,"-{0,1}\\d+.\\d+")
# grabs the first digit and neg. if needed

new_users_frame$lng[matches] <- str_extract(extract_locations[matches]
                                            ,"-{0,1}\\d+.\\d+$")
# grabs the second digit and neg. if needed

new_users_frame <- new_users_frame %>%
  mutate(lat = as.numeric(lat), lng = as.numeric(lng))
#ignore warning, it NA's one value that shouldn't have been grabbed
```

```{r LoadData}
data <- read_csv("tweets20150101-20190705.csv")
data <- data %>% rename(linenumber = X1)
```

```{r Language}
lang <- detect_language(data$text)
summary(as.factor(lang))

eng_data <- data %>% filter(lang == "en")
```

```{r Tokenize}
data <- data %>% unnest_tokens(words, text, drop=TRUE)
eng_data <- eng_data %>% unnest_tokens(words, text, drop=TRUE)

# Weird case where Muslims are using #sake to represent 'for the sake of Allah'
religious <- data$words %in% c("allah","Allah","Allah's","allah's"
                               ,"judgement", "Judgement", "prophet"
                               ,"Prophet", "prophets","Prophets"
                               ,"Prophet's","prophet's","martyr"
                               ,"Martyr","Martyrs","martyrs"
                               ,"sacrificing","Sacrificing"
                               ,"piety","Piety")
religious_lines <- unique(data[religious,"linenumber"])$linenumber
# OR
#religious_lines <- unique(data[religious,"id"])$id

# find out which users are muslim, use this to delete any tweets associated
#       with them (probably not drinking sake)
# remove religious lines from the data
muslim_users <- unique(data[data$linenumber %in% religious_lines,"user"])
data <- data %>% filter(!(linenumber %in% religious_lines))
# or may need:
#data <- data %>% filter(!(id %in% religious_lines))
eng_data <- eng_data %>% filter(!(linenumber %in% religious_lines))
```

```{r Stopwords}
# Note that this only deals with English and Japanese
remove <- pull(get_stopwords(), 1) # grab first column
ja_stopwords <- read_csv("stopwords-ja/stopwords-ja.txt", col_names = "words")
remove <- c(remove, c("I'll", "I'm", "I'd", "I've")) # missing uppercase
remove <- tibble::enframe(remove, name = NULL) # tibble vector
names(remove) <- "words"

data <- data %>% 
        anti_join(remove, by = "words") %>%
        anti_join(ja_stopwords, by = "words")
eng_data <- eng_data %>% 
        anti_join(remove, by = "words")
```

```{r WordCounts}
unigrams_all <- data %>% count(words, sort = TRUE)
unigrams_eng <- eng_data %>% count(words, sort = TRUE)
```

```{r Wordclouds}
# remove some ugly words for the cloud
excission_list <- c("p", "w", "https", "http", "www.instagram.com",
        "pic.twitter.com", "1","2","3","4","5","ig_twitter_share", "ow.ly", "ift.tt",
        "utm_source","instagram.com","twitter.com","bit.ly","fb.me","6","7","8",
        "9","10","0","buff.ly","rt","2shmqcv","2015","igshid","foodporn","s",
        "2016","2018","2017","goo.gl","www.meigennavi.net", "sake.oh.land.to",
        "meisyu.net","monipla.com","fgo","100","htm","ar954","3d","html","a.r10"
        ,"20","00","17","tw","followme","download","word","tbsradio"
        ,"please.thank","sawanotsuru_campaign","tv","de","jp"
        ,"itunes.apple.com", "13651", "12", "15","12730","11","13","14"
        ,"saké", "30")
excise <- !(unigrams_eng$words %in% excission_list)
abridged_eng_counts <- unigrams_eng[excise,]
excise <- !(unigrams_all$words %in% excission_list)
abridged_all_counts <- unigrams_all[excise,]

# correct some of the international unigrams to bigrams
#       ex. "japanesetemple" -> "Japanese temple"
correction_list <-c(
        "japanesetemple" = "Japanese temple"
        ,"japaneseshrine" = "Japanese shrine"
        ,"japanesetradition" = "Japanese tradition"
        ,"japan" = "Japan"
        ,"japanesesake" = "Japanese sake"
        ,"shushusake" = "shushu sake"
        ,"japanesefood" = "Japanese food"
        ,"japanese" = "Japanese"
        ,"tokyo" = "Tokyo"
        ,"sake" = "saké"
)

for(word in abridged_all_counts$words){# go through whole list
        if(word %in% names(correction_list)){# if in the correction list
                index <- which(abridged_all_counts$words == word)# replace it
                abridged_all_counts$words[index] <- correction_list[word]
        }
}

#png("FILE NAME", height = 4, width = 4, units = "in", res = 400)
par(family = 'STSong')
wordcloud(abridged_all_counts$words, abridged_all_counts$n, 
          max.words = 200, scale = c(4.2, 0.36),
          colors = RColorBrewer::brewer.pal(9, "RdPu")[6:9],
          random.order = FALSE)
#dev.off()
```